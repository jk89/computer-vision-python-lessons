{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(master=\"spark://10.0.0.3:6060\")\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://10.0.0.3:4040/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1062686, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #maths\n",
    "visualFeatureVocabulary = None\n",
    "visualFeatureVocabularyList = None\n",
    "with open(\"data/ORBvoc.txt\", \"r\") as fin:\n",
    "    extractedFeatures = list(map(lambda x: x.split(\" \")[2:-2], fin.readlines()[1:]))\n",
    "    dedupedFeatureStrings = set()\n",
    "    for extractedFeature in extractedFeatures:\n",
    "        strRep = \".\".join(extractedFeature)\n",
    "        dedupedFeatureStrings.add(strRep)\n",
    "    finalFeatures = []\n",
    "    for dedupedFeatureStr in list(dedupedFeatureStrings):\n",
    "        finalFeatures.append([int(i) for i in dedupedFeatureStr.split(\".\")])\n",
    "    visualFeatureVocabulary = np.asarray(finalFeatures, dtype=np.uint8)\n",
    "    visualFeatureVocabularyList  = list(finalFeatures)\n",
    "print(visualFeatureVocabulary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector metrics\n",
    "def hammingVector(stack1, stack2):\n",
    "    return (stack1 != stack2).sum() #.sum(axis=1)\n",
    "def euclideanVector(stack1, stack2):\n",
    "    return (np.absolute(stack2-stack1)).sum() #.sum(axis=1)\n",
    "\n",
    "# point metrics\n",
    "def euclideanPoint(p1, p2): \n",
    "    return np.sum((p1 - p2)**2) \n",
    "def hammingPoint(p1, p2): \n",
    "    return np.sum((p1 != p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.parallelize (\n",
    "   visualFeatureVocabularyList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = words.zipWithIndex().map(lambda xy: (xy[1],xy[0])).toDF([\"id\", \"vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def nearestCenteroid(dataIdValue, centeroidIdValues, metric):\n",
    "    import numpy as np\n",
    "    dataId, dataValue = dataIdValue\n",
    "    dataNp = np.asarray(dataValue)\n",
    "    distances = []\n",
    "    for centeroidIdValue in centeroidIdValues:\n",
    "        centeroidId, centeroidValue = centeroidIdValue\n",
    "        centeroidNp = np.asarray(centeroidValue)\n",
    "        distance = metric(dataNp, centeroidNp)\n",
    "        distances.append(distance)\n",
    "        #distances.append((dataNp != centeroidNp).sum())\n",
    "    distances = np.asarray(distances)\n",
    "    closestCenteroid = np.argmin(distances)\n",
    "    return int(closestCenteroid)\n",
    "\n",
    "def optimiseClusterMembershipSpark(data, dataFrame, n=4, metric=hammingVector, intitalClusterIndices=None):\n",
    "    dataShape = data.shape\n",
    "    dataRDD = dataFrame.rdd\n",
    "    lengthOfData = dataShape[0]\n",
    "    if intitalClusterIndices is None:\n",
    "        index = np.random.choice(lengthOfData, n, replace=False)\n",
    "    else:\n",
    "        index = intitalClusterIndices\n",
    "    listIndex = [int(i) for i in list(index)]\n",
    "    centeroidIdValues = [(i,data[index[i]]) for i in range(len(index))]\n",
    "    dataRDD = dataRDD.filter(lambda dataIdValue: int(dataIdValue[\"id\"]) not in listIndex)\n",
    "    associatedClusterPoints = dataRDD.map(lambda dataIdValue: (dataIdValue[0],nearestCenteroid(dataIdValue, centeroidIdValues, metric)))\n",
    "    clusters = associatedClusterPoints.toDF([\"id\", \"bestC\"]).groupBy(\"bestC\").agg(F.collect_list(\"id\").alias(\"cluster\"))\n",
    "    return index, clusters\n",
    "    \n",
    "ret = optimiseClusterMembershipSpark(visualFeatureVocabulary, vocab, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ret[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataB = sc.broadcast(visualFeatureVocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return hammingPoint(np.asarray(p1), np.asarray(p2))\n",
    "\n",
    "def costKernel(testCenteroid, clusterData, metric):\n",
    "    data = dataB.value\n",
    "    cluster = np.asarray(clusterData)\n",
    "    lenCluster = cluster.shape[0]\n",
    "    lenFeature = data.shape[1]\n",
    "    testCenteroidColumn = np.zeros(shape=(lenCluster, lenFeature), dtype=data.dtype)\n",
    "    newClusterColumn = np.zeros(shape=(lenCluster, lenFeature), dtype=data.dtype)\n",
    "    for i in range(0, lenCluster):\n",
    "        newClusterColumn[i] = data[cluster[i]]\n",
    "        testCenteroidColumn[i] = data[int(testCenteroid)] \n",
    "    pairwiseDistance = metric(newClusterColumn, testCenteroidColumn)#(newClusterColumn != testCenteroidColumn).sum(axis=1)\n",
    "    cost = np.sum(pairwiseDistance)\n",
    "    return float(cost)\n",
    "\n",
    "def optimiseCentroidSelectionSpark(centeroids, clustersFrames, data, dataFrame, metric=euclideanVector):\n",
    "    dataRDD = dataFrame.rdd\n",
    "    dataShape = data.shape\n",
    "    newCenteroidIds = []\n",
    "    totalCost = 0\n",
    "    for clusterIdx in range(len(centeroids)):\n",
    "        print(\"clusterIdx\", clusterIdx)\n",
    "        oldCenteroid = centeroids[clusterIdx]\n",
    "        clusterFrame = clustersFrames.filter(clustersFrames.bestC == clusterIdx).select(F.explode(clustersFrames.cluster))\n",
    "        clusterData = clusterFrame.collect()[0]\n",
    "        cluster = np.asarray(clusterData)\n",
    "        costData = clusterFrame.rdd.map(lambda pointId: (pointId[0], costKernel(pointId[0], clusterData, metric)))\n",
    "        cost = costData.map(lambda pointIdCost: pointIdCost[1]).sum()\n",
    "        totalCost = totalCost + cost\n",
    "        bestPoint = costData.sortBy(lambda pointId_Cost: pointId_Cost[1]).take(1)[0][0]\n",
    "        newCenteroidIds.append(bestPoint)\n",
    "    return (newCenteroidIds, totalCost)\n",
    "\n",
    "\n",
    "newCentroids, cost = optimiseCentroidSelectionSpark(ret[0], ret[1], visualFeatureVocabulary, vocab)\n",
    "print(\"newCentroids, cost\", (newCentroids, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ret[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def seedClusters(data, k, metric=euclideanPoint): \n",
    "    centeroids = list(np.random.choice(data.shape[0], 1, replace=False)) \n",
    "    for _ in range(k - 1):\n",
    "        distances = []\n",
    "        for i in range(data.shape[0]): \n",
    "            point = data[i, :] \n",
    "            minDistance = sys.maxsize\n",
    "            for j in range(len(centeroids)): \n",
    "                distance = metric(point, data[centeroids[j]]) \n",
    "                minDistance = min(minDistance, distance) \n",
    "            distances.append(minDistance) \n",
    "        distances = np.array(distances) \n",
    "        centeroids.append(np.argmax(distances)) \n",
    "        distances = [] \n",
    "    return centeroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastClusters = None\n",
    "def clusterOpt(data, dataFrame, nRegions, pointMetric, vectorMetric):\n",
    "    print(\"clusterOpt\")\n",
    "    # define a routine to keep going until cost stays the same or gets worse\n",
    "    #get seeds\n",
    "    seeds = seedClusters(data, nRegions, pointMetric)\n",
    "    print(seeds)\n",
    "    lastCenteroids, lastClusters = optimiseClusterMembershipSpark(data, dataFrame, nRegions, vectorMetric, seeds)\n",
    "    lastCost = float('inf')\n",
    "    iteration = 0\n",
    "    escape = False\n",
    "    while not escape:\n",
    "        iteration = iteration + 1\n",
    "        currentCenteroids, currentCost = optimiseCentroidSelectionSpark(lastCenteroids, lastClusters, data, dataFrame, vectorMetric)\n",
    "        currentCenteroids, currentClusters = optimiseClusterMembershipSpark(data, dataFrame, nRegions, vectorMetric, currentCenteroids)\n",
    "        print((currentCost<lastCost, currentCost, lastCost, currentCost - lastCost))\n",
    "        if (currentCost<lastCost):\n",
    "            print((\"iteration\",iteration,\"cost improving...\", currentCost, lastCost))\n",
    "            lastCost = currentCost\n",
    "            lastCenteroids = currentCenteroids\n",
    "            lastClusters = currentClusters\n",
    "        else:\n",
    "            print((\"iteration\",iteration,\"cost got worse or did not improve\", currentCost, lastCost))\n",
    "            escape = True\n",
    "        print(\"--------------------\")\n",
    "    return (lastCenteroids, lastClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "(lastCenteroids, lastClusters) = clusterOpt(visualFeatureVocabulary, vocab, 8, hammingPoint, hammingVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastCenteroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastClusters.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import sys\n",
    "from numba import jit, prange\n",
    "\n",
    "@jit(nopython=True, nogil=True, parallel=True) #'void(double[:], double[:], double[:])', \n",
    "def seedClusters(data, k, metric): \n",
    "    centeroids = np.random.choice(data.shape[0], k, replace=False) #list(np.random.choice(data.shape[0], 1, replace=False)) \n",
    "    for _ in prange(k - 1):\n",
    "        distances = np.zeros(shape=data.shape, dtype=np.float64)#[]\n",
    "        for i in range(data.shape[0]): \n",
    "            point = data[i, :] \n",
    "            minDistance = sys.maxsize\n",
    "            for j in range(len(centeroids)): \n",
    "                distance = metric(point, data[centeroids[j]]) \n",
    "                minDistance = min(minDistance, distance) \n",
    "            distances[i] = minDistance#distances.append(minDistance) \n",
    "        #distances = np.array(distances, dtype=np.float64) \n",
    "        centeroids[_ + 1] = np.argmax(distances)\n",
    "        #centeroids.append(np.argmax(distances)) \n",
    "        #distances = np.zeros(shape=data.shape, dtype=data.dtype)#[] \n",
    "    return centeroids\n",
    "\n",
    "def nearestCenteroid(dataIdValue, centeroidIdValues, metric):\n",
    "    import numpy as np\n",
    "    dataId, dataValue = dataIdValue\n",
    "    dataNp = np.asarray(dataValue)\n",
    "    distances = []\n",
    "    for centeroidIdValue in centeroidIdValues:\n",
    "        centeroidId, centeroidValue = centeroidIdValue\n",
    "        centeroidNp = np.asarray(centeroidValue)\n",
    "        distance = metric(dataNp, centeroidNp)\n",
    "        distances.append(distance)\n",
    "    distances = np.asarray(distances)\n",
    "    closestCenteroid = np.argmin(distances)\n",
    "    return int(closestCenteroid)\n",
    "\n",
    "def optimiseClusterMembershipSpark(data, dataFrame, n, metric, intitalClusterIndices=None):\n",
    "    dataShape = data.shape\n",
    "    dataRDD = dataFrame.rdd\n",
    "    lengthOfData = dataShape[0]\n",
    "    if intitalClusterIndices is None:\n",
    "        index = np.random.choice(lengthOfData, n, replace=False)\n",
    "    else:\n",
    "        index = intitalClusterIndices\n",
    "    listIndex = [int(i) for i in list(index)]\n",
    "    centeroidIdValues = [(i,data[index[i]]) for i in range(len(index))]\n",
    "    dataRDD = dataRDD.filter(lambda dataIdValue: int(dataIdValue[\"id\"]) not in listIndex)\n",
    "    associatedClusterPoints = dataRDD.map(lambda dataIdValue: (dataIdValue[0],nearestCenteroid(dataIdValue, centeroidIdValues, metric)))\n",
    "    clusters = associatedClusterPoints.toDF([\"id\", \"bestC\"]).groupBy(\"bestC\").agg(F.collect_list(\"id\").alias(\"cluster\"))\n",
    "    return index, clusters\n",
    "\n",
    "def costKernel(data, testCenteroid, clusterData, metric):\n",
    "    cluster = np.asarray(clusterData)\n",
    "    lenCluster = cluster.shape[0]\n",
    "    lenFeature = data.shape[1]\n",
    "    testCenteroidColumn = np.zeros(shape=(lenCluster, lenFeature), dtype=data.dtype)\n",
    "    newClusterColumn = np.zeros(shape=(lenCluster, lenFeature), dtype=data.dtype)\n",
    "    for i in range(0, lenCluster):\n",
    "        newClusterColumn[i] = data[cluster[i]]\n",
    "        testCenteroidColumn[i] = data[int(testCenteroid)] \n",
    "    pairwiseDistance = metric(newClusterColumn, testCenteroidColumn)\n",
    "    cost = np.sum(pairwiseDistance)\n",
    "    return float(cost)\n",
    "\n",
    "def optimiseCentroidSelectionSpark(data, dataFrame, centeroids, clustersFrames, metric):\n",
    "    dataRDD = dataFrame.rdd\n",
    "    dataShape = data.shape\n",
    "    newCenteroidIds = []\n",
    "    totalCost = 0\n",
    "    for clusterIdx in range(len(centeroids)):\n",
    "        print(\"clusterIdx\", clusterIdx)\n",
    "        oldCenteroid = centeroids[clusterIdx]\n",
    "        clusterFrame = clustersFrames.filter(clustersFrames.bestC == clusterIdx).select(F.explode(clustersFrames.cluster))\n",
    "        clusterData = clusterFrame.collect()[0]\n",
    "        cluster = np.asarray(clusterData)\n",
    "        costData = clusterFrame.rdd.map(lambda pointId: (pointId[0], costKernel(data, pointId[0], clusterData, metric)))\n",
    "        cost = costData.map(lambda pointIdCost: pointIdCost[1]).sum()\n",
    "        totalCost = totalCost + cost\n",
    "        bestPoint = costData.sortBy(lambda pointId_Cost: pointId_Cost[1]).take(1)[0][0]\n",
    "        newCenteroidIds.append(bestPoint)\n",
    "    return (newCenteroidIds, totalCost)\n",
    "\n",
    "def clusterOpt(data, dataFrame, nRegions, pointMetric, vectorMetric):\n",
    "    # define a routine to keep going until cost stays the same or gets worse\n",
    "    #get seeds\n",
    "    seeds = seedClusters(data, nRegions, pointMetric)\n",
    "    print(seeds)\n",
    "    lastCenteroids, lastClusters = optimiseClusterMembershipSpark(data, dataFrame, nRegions, vectorMetric, seeds)\n",
    "    lastCost = float('inf')\n",
    "    iteration = 0\n",
    "    escape = False\n",
    "    while not escape:\n",
    "        iteration = iteration + 1\n",
    "        currentCenteroids, currentCost = optimiseCentroidSelectionSpark(lastCenteroids, lastClusters, data, dataFrame, vectorMetric)\n",
    "        currentCenteroids, currentClusters = optimiseClusterMembershipSpark(data, dataFrame, nRegions, vectorMetric, currentCenteroids)\n",
    "        print((currentCost<lastCost, currentCost, lastCost, currentCost - lastCost))\n",
    "        if (currentCost<lastCost):\n",
    "            print((\"iteration\",iteration,\"cost improving...\", currentCost, lastCost))\n",
    "            lastCost = currentCost\n",
    "            lastCenteroids = currentCenteroids\n",
    "            lastClusters = currentClusters\n",
    "        else:\n",
    "            print((\"iteration\",iteration,\"cost got worse or did not improve\", currentCost, lastCost))\n",
    "            escape = True\n",
    "        print(\"--------------------\")\n",
    "    return (lastCenteroids, lastClusters)\n",
    "\n",
    "#vector metrics\n",
    "def hammingVector(stack1, stack2):\n",
    "    return (stack1 != stack2).sum() #.sum(axis=1)\n",
    "def euclideanVector(stack1, stack2):\n",
    "    return (np.absolute(stack2-stack1)).sum() #.sum(axis=1)\n",
    "# point metrics\n",
    "@jit(nopython=True, nogil=True)\n",
    "def euclideanPoint(p1, p2): \n",
    "    return np.sum((p1 - p2)**2) \n",
    "@jit(nopython=True, nogil=True)\n",
    "def hammingPoint(p1, p2): \n",
    "    return np.sum((p1 != p2))\n",
    "\n",
    "def fit(sc, data, nRegions = 2, metric = \"euclidean\", seeding = \"heuristic\"):\n",
    "    if metric == \"euclidean\":\n",
    "        pointMetric = euclideanPoint\n",
    "        vectorMetric = euclideanVector\n",
    "    elif metric == \"hamming\":\n",
    "        pointMetric = hammingPoint\n",
    "        vectorMetric = hammingVector\n",
    "    else:\n",
    "        print(\"unsuported metric\")\n",
    "        return\n",
    "\n",
    "    dataN = np.asarray(data)\n",
    "    seeds = None\n",
    "    if (seeding == \"heuristic\"):\n",
    "        seeds = list(seedClusters(dataN, nRegions, pointMetric))\n",
    "    dataFrame  = sc.parallelize(data).zipWithIndex().map(lambda xy: (xy[1],xy[0])).toDF([\"id\", \"vector\"])\n",
    "    lastCenteroids, lastClusters = optimiseClusterMembershipSpark(dataN, dataFrame, nRegions, vectorMetric, seeds)\n",
    "    lastCost = float('inf')\n",
    "    iteration = 0\n",
    "    escape = False\n",
    "    while not escape:\n",
    "        iteration = iteration + 1\n",
    "        currentCenteroids, currentCost = optimiseCentroidSelectionSpark(dataN, dataFrame, lastCenteroids, lastClusters, vectorMetric)\n",
    "        currentCenteroids, currentClusters = optimiseClusterMembershipSpark(dataN, dataFrame, nRegions, vectorMetric, currentCenteroids)\n",
    "        print((currentCost<lastCost, currentCost, lastCost, currentCost - lastCost))\n",
    "        if (currentCost<lastCost):\n",
    "            print((\"iteration\",iteration,\"cost improving...\", currentCost, lastCost))\n",
    "            lastCost = currentCost\n",
    "            lastCenteroids = currentCenteroids\n",
    "            lastClusters = currentClusters\n",
    "        else:\n",
    "            print((\"iteration\",iteration,\"cost got worse or did not improve\", currentCost, lastCost))\n",
    "            escape = True\n",
    "    return (lastCenteroids, lastClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = fit(sc, visualFeatureVocabularyList, 4, \"hamming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n",
      "clusterIdx 0\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "ret = fit(sc, visualFeatureVocabularyList, 2, \"hamming\")\n",
    "#ret = KMedoids.fit(sc, visualFeatureVocabularyList, 4, \"hamming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv2",
   "language": "python",
   "name": "cv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
