{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(master=\"spark://10.0.0.3:6060\")\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://10.0.0.3:4040/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pyspark\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def seedKernel(data, dataIdValue, centroids, k, metric):\n",
    "    point = dataIdValue[1]\n",
    "    d = sys.maxsize \n",
    "    for j in range(len(centroids)): \n",
    "        temp_dist = metric(point, data[centroids[j]]) \n",
    "        d = min(d, temp_dist) \n",
    "    return int(d)\n",
    "\n",
    "def seedClusters(data, dataFrame, k, metric):\n",
    "    centroids = list(np.random.choice(data.shape[0], 1, replace=False))\n",
    "    for i in range(k - 1):\n",
    "        print(\"clusterSeed\", i)\n",
    "        dist = []\n",
    "        mK = dataFrame.rdd.map(lambda dataIdValue: seedKernel(data, dataIdValue, centroids, k, metric))\n",
    "        mK_collect = mK.collect()\n",
    "        dist = np.array(mK_collect) \n",
    "        next_centroid = np.argmax(dist)\n",
    "        centroids.append(next_centroid) \n",
    "        dist = []\n",
    "    return centroids \n",
    "\n",
    "def nearestCenteroidKernel(dataIdValue, centeroidIdValues, metric):\n",
    "    dataId, dataValue = dataIdValue\n",
    "    dataNp = np.asarray(dataValue)\n",
    "    distances = []\n",
    "    for centeroidId, centeroidValue in centeroidIdValues:\n",
    "        centeroidNp = np.asarray(centeroidValue)\n",
    "        distance = metric(dataNp, centeroidNp)\n",
    "        distances.append(distance)\n",
    "    distances = np.asarray(distances)\n",
    "    closestCenteroid = np.argmin(distances)\n",
    "    return int(closestCenteroid)\n",
    "\n",
    "def optimiseClusterMembershipSpark(data, dataFrame, n, metric, intitalClusterIndices=None):\n",
    "    dataShape = data.shape\n",
    "    dataRDD = dataFrame.rdd\n",
    "    lengthOfData = dataShape[0]\n",
    "    if intitalClusterIndices is None:\n",
    "        index = np.random.choice(lengthOfData, n, replace=False)\n",
    "    else:\n",
    "        index = intitalClusterIndices\n",
    "    listIndex = [int(i) for i in list(index)]\n",
    "    centeroidIdValues = [(i,data[index[i]]) for i in range(len(index))]\n",
    "    dataRDD = dataRDD.filter(lambda dataIdValue: int(dataIdValue[\"id\"]) not in listIndex)\n",
    "    associatedClusterPoints = dataRDD.map(lambda dataIdValue: (dataIdValue[0],nearestCenteroidKernel(dataIdValue, centeroidIdValues, metric)))\n",
    "    clusters = associatedClusterPoints.toDF([\"id\", \"bestC\"]).groupBy(\"bestC\").agg(F.collect_list(\"id\").alias(\"cluster\"))\n",
    "    return index, clusters\n",
    "\n",
    "def costKernel(data, testCenteroid, clusterData, metric):\n",
    "    cluster = np.asarray(clusterData)\n",
    "    lenCluster = cluster.shape[0]\n",
    "    lenFeature = data.shape[1]\n",
    "    testCenteroidColumn = np.zeros(shape=(lenCluster, lenFeature), dtype=data.dtype)\n",
    "    newClusterColumn = np.zeros(shape=(lenCluster, lenFeature), dtype=data.dtype)\n",
    "    for i in range(0, lenCluster):\n",
    "        newClusterColumn[i] = data[cluster[i]]\n",
    "        testCenteroidColumn[i] = data[int(testCenteroid)] \n",
    "    pairwiseDistance =  metric(newClusterColumn, testCenteroidColumn)# (np.absolute(newClusterColumn-testCenteroidColumn).sum(axis=1))# metric(newClusterColumn, testCenteroidColumn)\n",
    "    cost = np.sum(pairwiseDistance)\n",
    "    return float(cost) #newClusterColumn.shape[1]\n",
    "\n",
    "def optimiseCentroidSelectionSpark(data, dataFrame, centeroids, clustersFrames, metric):\n",
    "    dataRDD = dataFrame.rdd\n",
    "    dataShape = data.shape\n",
    "    newCenteroidIds = []\n",
    "    totalCost = 0\n",
    "    for clusterIdx in range(len(centeroids)):\n",
    "        print(\"clusterOpIdx\", clusterIdx)\n",
    "        oldCenteroid = centeroids[clusterIdx]\n",
    "        clusterFrame = clustersFrames.filter(clustersFrames.bestC == clusterIdx).select(F.explode(clustersFrames.cluster))\n",
    "        clusterData = clusterFrame.collect()\n",
    "        if clusterData:\n",
    "            clusterData = [clusterData[i].col for i in range(len(clusterData))]\n",
    "        else:\n",
    "            clusterData = []\n",
    "        cluster = np.asarray(clusterData)\n",
    "        costData = clusterFrame.rdd.map(lambda pointId: (pointId[0], costKernel(data, pointId[0], clusterData, metric)))\n",
    "        cost = costData.map(lambda pointIdCost: pointIdCost[1]).sum()\n",
    "        totalCost = totalCost + cost\n",
    "        pointResult = costData.sortBy(lambda pointId_Cost: pointId_Cost[1]).take(1)\n",
    "        if (pointResult):\n",
    "            bestPoint = pointResult[0][0]\n",
    "        else:\n",
    "            bestPoint = oldCenteroid\n",
    "        newCenteroidIds.append(bestPoint)\n",
    "    return (newCenteroidIds, totalCost)\n",
    "\n",
    "#vector metrics\n",
    "def hammingVector(stack1, stack2):\n",
    "    return (stack1 != stack2).sum(axis=1)\n",
    "def euclideanVector(stack1, stack2):\n",
    "    return (np.absolute(stack2-stack1)).sum(axis=1)\n",
    "# point metrics\n",
    "def euclideanPoint(p1, p2): \n",
    "    return np.sum((p1 - p2)**2) \n",
    "def hammingPoint(p1, p2): \n",
    "    return np.sum((p1 != p2))\n",
    "\n",
    "def fit(sc, data, nRegions = 2, metric = \"euclidean\", seeding = \"heuristic\"):\n",
    "    if metric == \"euclidean\":\n",
    "        pointMetric = euclideanPoint\n",
    "        vectorMetric = euclideanVector\n",
    "    elif metric == \"hamming\":\n",
    "        pointMetric = hammingPoint\n",
    "        vectorMetric = hammingVector\n",
    "    else:\n",
    "        print(\"unsuported metric\")\n",
    "        return\n",
    "\n",
    "    dataN = np.asarray(data)\n",
    "    seeds = None\n",
    "    dataFrame  = sc.parallelize(data).zipWithIndex().map(lambda xy: (xy[1],xy[0])).toDF([\"id\", \"vector\"]).cache()\n",
    "    if (seeding == \"heuristic\"):\n",
    "        seeds = list(seedClusters(dataN, dataFrame, nRegions, pointMetric))\n",
    "    lastCenteroids, lastClusters = optimiseClusterMembershipSpark(dataN, dataFrame, nRegions, pointMetric, seeds)\n",
    "    lastCost = float('inf')\n",
    "    iteration = 0\n",
    "    escape = False\n",
    "    while not escape:\n",
    "        iteration = iteration + 1\n",
    "        currentCenteroids, currentCost = optimiseCentroidSelectionSpark(dataN, dataFrame, lastCenteroids, lastClusters, vectorMetric)\n",
    "        currentCenteroids, currentClusters = optimiseClusterMembershipSpark(dataN, dataFrame, nRegions, pointMetric, currentCenteroids)\n",
    "        print((currentCost<lastCost, currentCost, lastCost, currentCost - lastCost))\n",
    "        if (currentCost<lastCost):\n",
    "            print((\"iteration\",iteration,\"cost improving...\", currentCost, lastCost))\n",
    "            lastCost = currentCost\n",
    "            lastCenteroids = currentCenteroids\n",
    "            lastClusters = currentClusters\n",
    "        else:\n",
    "            print((\"iteration\",iteration,\"cost got worse or did not improve\", currentCost, lastCost))\n",
    "            escape = True\n",
    "    bc = bestClusters.collect()\n",
    "    unpackedClusters = [bc[i].cluster for i in range(len(bc))]\n",
    "    return (lastCenteroids, unpackedClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1062686, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np #maths\n",
    "visualFeatureVocabulary = None\n",
    "visualFeatureVocabularyList = None\n",
    "with open(\"data/ORBvoc.txt\", \"r\") as fin:\n",
    "    extractedFeatures = list(map(lambda x: x.split(\" \")[2:-2], fin.readlines()[1:]))\n",
    "    dedupedFeatureStrings = set()\n",
    "    for extractedFeature in extractedFeatures:\n",
    "        strRep = \".\".join(extractedFeature)\n",
    "        dedupedFeatureStrings.add(strRep)\n",
    "    finalFeatures = []\n",
    "    for dedupedFeatureStr in list(dedupedFeatureStrings):\n",
    "        finalFeatures.append([int(i) for i in dedupedFeatureStr.split(\".\")])\n",
    "    visualFeatureVocabulary = np.asarray(finalFeatures, dtype=np.uint8)\n",
    "    visualFeatureVocabularyList  = list(finalFeatures)\n",
    "print(visualFeatureVocabulary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ret = fit(sc, visualFeatureVocabularyList, 4, \"hamming\")\n",
    "#ret = KMedoids.fit(sc, visualFeatureVocabularyList, 4, \"hamming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ret[1].show()\n",
    "#ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustering.cluster import cluster_visualizer\n",
    "from pyclustering.utils import read_sample\n",
    "from pyclustering.samples.definitions import FCPS_SAMPLES\n",
    "from pyclustering.samples.definitions import SIMPLE_SAMPLES\n",
    "sample = read_sample(FCPS_SAMPLES.SAMPLE_GOLF_BALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusterSeed 0\n",
      "clusterSeed 1\n",
      "clusterSeed 2\n",
      "clusterSeed 3\n",
      "clusterSeed 4\n",
      "clusterSeed 5\n",
      "clusterSeed 6\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#visualizer = cluster_visualizer()\n",
    "bestCentroids, bestClusters = fit(sc, visualFeatureVocabularyList, 100) #\"hamming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizer.append_clusters(bestClusters, sample)\n",
    "#visualizer.show()\n",
    "#print(bestClustersData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv2",
   "language": "python",
   "name": "cv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
